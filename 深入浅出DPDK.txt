第一部分　DPDK基础篇第1章　认识DPDK第2章　Cache和内存1）RAM（Random Access Memory）：随机访问存储器。
2）SRAM（Static RAM）：静态随机访问存储器。
3）DRAM（Dynamic RAM）：动态随机访问存储器。
4）SDRAM（Synchronous DRAM）：同步动态随机访问存储器。
5）DDR（Double Data Rate SDRAM）：双数据速率SDRAM。
DPDK如何保证Cache一致性
解决Cache一致性问题的机制有两种：基于目录的协议（Directorybased protocol）和总线窥探协议（Bus snooping protocol）。其实还有另外一个Snarfing协议。
DPDK的解决方案很简单，首先就是避免多个核访问同一个内存地址或者数据结构。这样，每个核尽量都避免与其他核共享数据，从而减少因为错误的数据共享（cache line false sharing）导致的Cache一致性的开销。
TLB和大页
虚拟地址是指程序员使用虚拟地址进行编程，不用关心物理内存的大小，即使自己的程序出现了问题也不会影响其他程序的运行和系统的稳定。而处理器在寄存器收到虚拟地址之后，根据页表负责把虚拟地址转换成真正的物理地址。
如果没在TLB中匹配到逻辑地址，就出现TLB不命中，从而像我们刚才讨论的那样，进行常规的查找过程。如果TLB足够大，那么这个转换过程就会变得很快速。但是事实是，TLB是非常小的，一般都是几十项到几百项不等，并且为了提高命中率，很多处理器还采用全相连方式。另外，为了减少内存访问的次数，很多都采用回写的策略。
DDIO
DDIO技术是如何改进的呢？这种技术使外部网卡和CPU通过LLCCache直接交换数据，绕过了内存这个相对慢速的部件。这样，就增加了CPU处理网络报文的速度（减少了CPU和网卡等待内存的时间），减小了网络报文在服务器端的处理延迟。这样做也带来了一个问题，因为网络报文直接存储在LLC Cache中，这大大增加了对其容量的需求，因而在英特尔的E5处理器系列产品中，把LLC Cache的容量提高到了20MB。
第3章　并行计算多核性能和可扩展性
那随着核数的增加，性能是否能持续提升呢？Amdahl定律告诉我们，假设一个任务的工作量不变，多核并行计算理论时延加速上限取决于那些不能并行处理部分的比例。换句话说，多核并行计算下时延不能随着核数增加而趋于无限小。该定律明确告诉我们，利用多核处理器提升固定工作量性能的关键在于降低那些不得不串行部分占整个任务执行的比例。
亲和性
简单地说，CPU亲和性（Core affinity）就是一个特定的任务要在某个给定的CPU上尽量长时间地运行而不被迁移到其他处理器上的倾向性。这意味着线程可以不在处理器之间频繁迁移。这种状态正是我们所希望的，因为线程迁移的频率小就意味着产生的负载小。
线程独占
Linux内核提供了启动参数isolcpus。对于有4个CPU的服务器，在启动的时候加入启动参数isolcpus=2，3。那么系统启动后将不使用CPU3和CPU4。注意，这里说的不使用不是绝对地不使用，系统启动后仍然可以通过taskset命令指定哪些程序在这些核心中运行。
lcore的亲和性
默认情况下，lcore是与逻辑核一一亲和绑定的。带来性能提升的同时，也牺牲了一定的灵活性和能效。在现网中，往往有流量潮汐现象的发生，在网络流量空闲时，没有必要使用与流量繁忙时相同的核数。按需分配和灵活的扩展伸缩能力，代表了一种很有说服力的能效需求。于是，EAL pthread和逻辑核之间进而允许打破1：1的绑定关系，使得_lcore_id本身和CPU ID可以不严格一致。
SIMD是Single-Instruction Multiple-Data（单指令多数据）的缩写
第4章　同步互斥机制原子操作在DPDK代码中的定义都在rte_atomic.h文件中，主要包含两部分：内存屏蔽和原16、32和64位的原子操作API。
自旋锁使用时有两点需要注意：
1）自旋锁是不可递归的，递归地请求同一个自旋锁会造成死锁。
2）线程获取自旋锁之前，要禁止当前处理器上的中断。（防止获取锁的线程和中断形成竞争条件）
DPDK中自旋锁API的定义在rte_spinlock.h文件中，其中下面三个API被广泛的应用在告警、日志、中断机制、内存共享和link bonding的代码中，用于临界资源的保护。
Linux内核无锁环形缓冲
在Linux内核代码中，kfifo就是采用无锁环形缓冲的实现，kfifo是一种“First In First Out”数据结构，它采用了前面提到的环形缓冲区来实现，提供一个无边界的字节流服务。采用环形缓冲区的好处是，当一个数据元素被用掉后，其余数据元素不需要移动其存储位置，从而减少拷贝，提高效率。更重要的是，kfifo采用了并行无锁技术，kfifo实现的单生产/单消费模式的共享队列是不需要加锁同步的
第5章　报文转发·Packet input：报文输入。
·Pre-processing：对报文进行比较粗粒度的处理。
·Input classification：对报文进行较细粒度的分流。
·Ingress queuing：提供基于描述符的队列FIFO。
·Delivery/Scheduling：根据队列优先级和CPU状态进行调度。
·Accelerator：提供加解密和压缩/解压缩等硬件功能。
·Egress queueing：在出口上根据QOS等级进行调度。
·Post processing：后期报文处理释放缓存。
·Packet output：从硬件上发送出去。
第6章　PCIe与包处理I/OPCIe规范遵循开放系统互联参考模型（OSI），自上而下分为事务传输层、数据链路层、物理层，如图6-1a所示。PCIe一般作为处理器外部接口，把物理层朝PCIe根组件（Root Complex）方向的流量叫做上游流量（upstream或者inbound），反之叫做下游流量（downstream或者outbound）。
Mbuf与Mempool
Mbuf为了高效访问数据，DPDK将内存封装在Mbuf（struct rte_mbuf）结构体内。Mbuf主要用来封装网络帧缓存，也可用来封装通用控制信息缓存（缓存类型需使用CTRL_MBUF_FLAG来指定）。Mbuf结构报头经过精心设计，原先仅占1个Cache Line。随着Mbuf头部携带的信息越来越多，现在Mbuf头部已经调整成两个Cache Line，原则上将基础性、频繁访问的数据放在第一个Cache Line字节，而将功能性扩展的数据放在第二个Cache Line字节。Mbuf报头包含包处理所需的所有数据，对于单个Mbuf存放不下的巨型帧（Jumbo Frame），Mbuf还有指向下一个Mbuf结构的指针来形成帧链表结构。所有应用都应使用Mbuf结构来传输网络帧。
Mempool 多核CPU访问同一个内存池或者同一个环形缓存区时，因为每次读写时都要进行Compare-and-Set操作来保证期间数据未被其他核心修改，所以存取效率较低。DPDK的解决方法是使用单核本地缓存一部分数据，实时对环形缓存区进行块读写操作，以减少访问环形缓存区的次数。单核CPU对自己缓存的操作无须中断，访问效率因而得到提高。当然，这个方法也并非全是好处：该方法要求每个核CPU都有自己私用的缓存（大小可由用户定义，也可为0，或禁用该方法），而这些缓存在绝大部分时间都没有能得到百分之百运用，因此一部分内存空间将被浪费。
第7章　网卡性能优化DPDK的轮询模式
异步中断模式
轮询模式
混和中断轮询模式
Burst收发包就是DPDK的优化模式，它把收发包复杂的处理过程进行分解，打散成不同的相对较小的处理阶段，把相邻的数据访问、相似的数据运算集中处理。这样就能尽可能减少对内存或者低一级的处理器缓存的访问次数，用更少的访问次数来完成更多次收发包运算所需要数据的读或者写。
如果每次只收一个包，然后处理，最后再发送出去。那么在收一个包的时候发生内存访问或者低一级的处理器缓存访问的时候，往往会把临近的数据一并同步到处理器缓存中，因为处理器缓存更新都是按固定cache line（例如64字节）加载的。到中间计算的时候为了空出处理器缓存，把前面读取的数据又废弃了，下一次需要用到临近数据的时候又需要重新访问低一级处理器缓存，甚至是直接内存访问。而Burst模式收包则一次处理多个包，在第一次加载数据的时候，临近的数据可能恰好是下一个包需要用到的数据，这时候就可能会发生收两个包或者更多需要加载的数据只需要一次内存访问或者低级别的处理器缓存访问。这样就用更少的内存访问或者低级别的处理器缓存访问完成了更多个包的收包处理，总体上就节省了平均单个包收包所需的时间，提高了性能。
硬件平台对包处理性能的影响
在多处理器平台上，不同的PCIe插槽可能连接在不同的处理器上，跨处理器的PCIe设备访问会引入额外的CPU间通信，对性能的影响大。PCIe插槽与具体哪路CPU连接，在主板设计上基本确定，建议软件开发人员查找硬件手册来确定插槽属于哪个CPU节点。图7-10展示了一个典型的双路服务器上的PCIe插槽与处理器的连接关系，软件开发者可以咨询硬件工程师获得类似信息。如果看不到物理插卡的具体位置（较少场
合），一个简单的软件方法是查看PCIe地址中总线编号，如果小于80（例如0000：03：00.0），是连接到CPU0上，大于等于80（比如0000：81：00.0），则是连接在CPU1上。软件识别方法并不可靠，且会随着操作系统与驱动程序变化。
队列长度及各种阈值的设置
软件平台对包处理性能的影响(操作系统/BIOS的设置)，为了达到理想的负载均衡，必须要让每个队列都能均
匀地收到包，每个核都能均匀地负担收发包
第8章　流分类与多队列Linux内核对多队列的支持
先让我们以Linux kernel为例，来看看它是如何使用网卡多队列的特性。在这里先不考虑Linux socket的上层处理，假设数据包需要在kernel内部完成数据包的网络层转发，Linux NAPI和Qdisc技术已是被广泛应用的技术。
高级的网卡设备可以分析出包的类型，包的类型会携带在接收描述符中，应用程序可以根据描述符快速地确定包是哪种类型的包，避免了大量的解析包的软件开销。以Intel XL710为例，它可以分析出很多包的类型，比如传统的IP、TCP、UDP甚至VXLAN、NVGRE等tunnel报文，该信息可以体现在数据包的接收描述符中。对DPDK而言，Mbuf结构中含有相应的字段来表示网卡分析出的包的类型，从下面的代码可见Packet_type由二层、三层、四层及tunnel的信息来组成，应用程序可以很方便地定位到它需要处理的报文头部或是内容。
RSS
负载均衡是多队列网卡最常见的应用，其含义就是将负载分摊到多个执行单元上执行。对应Packet IO而言，就是将数据包收发处理分摊到多个核上。
Flow Director技术是Intel公司提出的根据包的字段精确匹配，将其分配到某个特定队列的技术。
QoS
多队列应用于服务质量（QoS）流量类别：把发送队列分配给不同的流量类别，可以让网卡在发送侧做调度；把收包队列分配给不同的流量类别，可以做到基于流的限速。根据流中优先级或业务类型字段，可以将流不同的业务类型有着不同的调度优先级及为其分配相应的带宽，一般网卡依照VLAN标签的UP（User Priority，用户优先级）字段。网卡依据UP字段，将流划分到某个业务类型（TC，Traffic Class），网卡设备根据TC对业务做相应的处理，比如确定相对应的队列，根据优先级调度等。

DPDK结合网卡Flow Director功能
一个设备需要一定的转发功能来处理数据平面的报文，同时需要处理一定量的控制报文。对于转发功能而言，要求较高的吞吐量，需要多个core来支持；对于控制报文的处理，其报文量并不大，但需要保证其可靠性，并且其处理逻辑也不同于转发逻辑。那么，我们就可以使用RSS来负载均衡其转发报文到多个核上，使用Flow Director将控制报文分配到指定的队列上，使用单独的核来处理。
DPDK结合网卡虚拟化及Cloud Filter功能
使用Linux的Ethtool工具，可以完成配置操作cloud filter，将大量的数据包直接分配到VF的队列中，交由运行在VF上的虚机应用来直接处理。如图8-13所示，使用这样的方法可以将一个网卡上的队列分配给DPDK和i40e Linux Kernel同时处理，很好地结合二者的优势。
可重构匹配表
可重构匹配表（Reconfigurable Match Table，RMT）是软件自定义网络（Software Defined Networking，SDN）中提出的用于配置转发平面的通用配置模型
第9章　硬件加速与功能卸载收包
网卡硬件会将4字节的VLAN tag从数据包中剥离，VLAN Tag中包含的信息对上层应用是有意义的，不能丢弃，此时，网卡硬件会在硬件描述符中设置两个域，将需要的信息通知驱动软件，包含此包是否曾被剥离了VLAN Tag以及被剥离的Tag。软件省去了剥离VLAN Tag的工作负荷，还获取了需要的信息。对应在DPDK软件，驱动对每个接收的数据包进行检测，会依据硬件描述符信息，如果剥离动作发生，需要将rte_mbuf数据结构中的PKT_RX_VLAN_PKT置位，表示已经接收到VLAN的报文，并且将被剥离VLAN Tag写入到下列字段。供上层应用处理。
发包
在发送端口插入VLAN在数据包中，是报文处理的常见操作。VLAN Tag由两部分组成：TPID（Tag Protocol Identifier），也就是VLAN的Ether type，和TCI（Tag Control Information）。TPID是一个固定的值，作为一个全局范围内起作用的值，可通过寄存器进行设置。而TCI是每个包相关的，需要逐包设置，在DPDK中，在调用发送函数前，必须提前设置mbuf数据结构，设置PKT_TX_VLAN_PKT位，同时将具体的Tag信息写入vlan_tci字段。

 QinQ
VLAN技术非常流行，使用也很广泛。早期定义时，VLAN本身只定义了12位宽，最多容纳4096个虚拟网络，这个限制使得它不适合大规模网络部署，为了解决这个局限，业界发展出了采用双层乃至多层VLAN堆叠模式，随着这种模式（也被称为QinQ技术）在网络应用中变得普遍
DPDK软件与offload性能优化
使用硬件来实现IEEE1588的功能，则能够快速完成获取并打时间戳的工作，能有效提高时间戳的精确性。
第二部分　DPDK虚拟化技术篇第10章　X86平台上的I/O虚拟化内存虚拟化的主要任务是实现地址空间的虚拟化，它引入了一层新的地址空间，即客户机物理地址空间。内存虚拟化通过两次地址转换来支持地址空间的虚拟化，即客户机虚拟地址GVA（Guest VirtualAddress）→客户机物理地址GPA（Guest Physical Address）→宿主机物理地址HPA（Host Physical Address）的转换。

I/O虚拟化包括管理虚拟设备和共享的物理硬件之间I/O请求的路由选择。目前，实现I/O虚拟化有三种方式：I/O全虚拟化、I/O半虚拟化和I/O透传。它们在处理客户机和宿主机通信以及宿主机和宿主机架构上分别采用了不同的处理方式。
I/O全虚拟化
宿主机截获客户机对I/O设备的访问请求，然后通过软件模拟真实的硬件。这种方式对客户机而言非常透明，无需考虑底层硬件的情况，不需要修改操作系统。宿主机必须从设备硬件的最底层开始模拟，尽管这样可以模拟得很彻底，以至于客户机操作系统完全不会感知到是运行在一个模拟环境中，但它的效率比较低。
I/O半虚拟化
半虚拟化虽然和全虚拟化一样，都是使用软件完成虚拟化工作，但是机制不一样。在全虚拟化中是所有对模拟I/O设备的访问都会造成VM-Exit，而在半虚拟化中是通过前后端驱动的协商，使数据传输中对共享内存的读写操作不会VM-Exit
I/O透传
直接把物理设备分配给虚拟机使用，例如直接分配一个硬盘或网卡给虚拟机，如图10-4c所示。这种方式需要硬件平台具备I/O透传技术，例如Intel VT-d技术。它能获得近乎本地的性能，并且CPU开销不高。

有了PCI/PCI-e透传技术，将物理网卡直接透传到虚拟机，虽然大大提高了虚拟机的吞吐量，但是一台服务器可用的物理网卡有限，如何才能实现水平拓展？因此，SR-IOV技术应运而生。
第11章　半虚拟化VirtioVirtio同I/O透传技术相比，目前在网络吞吐率、时延以及抖动上尚不具有优势，相关的优化工作正在进行当中。I/O透传的一个典型问题是从物理网卡接收到的数据包将直接到达客户机的接收队列，或者从客户机发送队列发出的包将直接到达其他客户机（比如同一个PF的VF）的接收队列或者直接从物理网卡发出，绕过了宿主机的参与；但在很多应用场景下，有需求要求网络包必须先经过宿主机的处理（如防火墙、负载均衡等），再传递给客户机。另外，I/O透传技术不能从硬件上支持虚拟机的动态迁移以及缺乏足够灵活的流分类规则。

第12章　加速包处理的vhost优化方案virtio-net的后端驱动经历过从virtio-net后端，到内核态vhost-net，再到用户态vhost-user的演进过程。其演进的过程是对性能的追求，导致其架构的变化。

Linux内核态的vhost-net模块需要在内核态完成报文拷贝和消息处理，这会给报文处理带来一定的性能损失，因此用户态的vhost应运而生。用户态vhost采用了共享内存技术，通过共享的虚拟队列来完成报文传输和控制，大大降低了vhost和virtio-net之间的数据传输成本。
DPDK vhost是用户态vhost的一种实现，其实现原理与Linux内核态vhost-net类似，它实现了用户态API，卸载了Qemu在Virtio-net上所承担的虚拟队列功能，同样基于Qemu共享内存空间布局、虚拟化队列的访问地址和事件文件描述符给用户态的vhost，使得vhost能进报文处理以及跟客户机通信。同时，由于报文拷贝在用户态进行，因此Linux内核的负担得到减轻。

virtio半虚拟化的性能优化不能仅仅只优化前端virtio或后端vhost，还需要两者同时优化，才能更好地提升性能。本章先介绍后端vhost演进之路，分析了各自架构的优缺点。然后重点介绍了DPDK在用户态vhost的设计思路以及优化点。最后，对如何使用DPDK进行vhost编程给出了简要示例。
第三部分　DPDK应用篇第13章　DPDK与网络功能虚拟化介绍DPDK与网络功能虚拟化（NFV）的关系，包括NFV的演进、DPDK优化VNF的方法，最后还给出几个成功的商业案例。
NFV架构框架包括以下主要模块：
·NFVI（网络功能虚拟化基础设施）
·VNF（虚拟网络功能）
·NFV M&O（NFV管理与编排）
DPDK主要关注在数据平面的典型场景中，在通用服务器上运行多个或者多种VNF，VNF作为虚拟网络功能（如前述的防火墙或者路由器），要能处理高速数据。

OPNFV建立之初，就希望最大限度地利用现有开源软件技术，启动了众多的项目，比如OpenDaylight、
OpenStack、Ceph存储、KVM、Open vSwitch以及Linux技术，这些大多数也已成为支撑云计算发展的主流开源软件。目前，OPNFV的主要项目展示在https://wiki.opnfv.org/。
第14章　Open　vSwitch（OVS）中的DPDK性能加速讲解Open vSwitch（OVS）中的DPDK性能加速。读者可以从这部分内容中了解到OVS的数据通路架构，DPDK如何支持OVS及其性能。

OVS最重要的组件是ovs-vswitchd，它实现了OpenFlow交换机的核心功能，并且通过Netlink协议直接和OVS的内部模块进行通信。用户通过ovs-ofctl可以使用OpenFlow协议去连接交换机并查询和控制。

ovs-vswitchd通过UNIX socket通信机制和ovsdb-server进程通信，将虚拟交换机的配置、流表、统计信息等保存在数据库ovsdb中。当用户需要和ovsdb-server通信以进行一些数据库操作时，可以通过运行ovsdbclient组件访问ovsdb-server，或者直接使用ovsdb-tool而不经ovsdb-server就对ovsdb数据库进行操作。

ovs-vsctl组件是一个用于交换机管理的基本工具，主要是获取或者更改ovs-vswitchd的配置信息，此工具操作的时候会更新ovsdb-server的数据库。同时，我们也可以通过另一个管理工具组件ovs-appctl发送一些内部命令给ovs-vswitchd以改变其配置。另外，在特定情况下，用户可能会需要自行管理运行在内核中的数据通路那么也可以通过调用ovsdpctl驱使ovs-vswitchd在不依赖于数据库的情况下去管理内核空间中的数据通路。

OVS在2.4版本中加入了DPDK的支持，作为一个编译选项，可以选用原始OVS还是DPDK加速的OVS。DPDK加速的OVS利用了DPDK的PMD驱动，向量指令，大页、绑核等技术，来优化用户态的数据通路，直接绕过内核态的数据通路，加速物理网口和虚拟网口的报文处理速度。

DPDK加速的OVS数据流转发的大致流程如下：
1）OVS的ovs-vswitchd接收到从OVS连接的某个网络端口发来的数据包，从数据包中提取源/目的IP、源/目的MAC、端口等信息。
2）OVS在用户态查看精确流表和模糊流表，如果命中，则直接转发。
3）如果还不命中，在SDN控制器接入的情况下，经过OpenFlow协议，通告给控制器，由控制器处理。
4）控制器下发新的流表，该数据包重新发起选路，匹配；报文转发，结束。

DPDK加速的OVS与原始OVS的区别在于，从OVS连接的某个网络
端口接收到的报文不需要openvswitch.ko内核态的处理，报文通过DPDK PMD驱动直接到达用户态ovs-vswitchd里。
对DPDK加速的OVS优化工作还在持续进行中，重点在用户态的转发逻辑（dpif）和vhost/virtio上，比如采用DPDK实现的cuckoo哈希算法替换原有的哈希算法去做流表查找，vhost后端驱动采用mbuf bulk分配的优化，等等。
第15章　基于DPDK的存储软件优化讨论DPDK在存储领域的应用，其中详细介绍了一套Intel提供的基于IA平台上的软件加速库和解决存储方案SPDK（StoragePerformance Development Kit），包括介绍了如何用DPDK来优化iSCSItarget的性能。最后介绍几种基于DPDK的用户态TCP/IP协议栈，包括libUNS、mTCP和OpenFastPath。

存储服务可以分为以下几类：文件存储（file）、块存储（block）和对象存储（object）。下面是几种常见的网络协议：
1）iSCSI协议：运行在以太网的SCSI（Small Computer SystemInterface，小型计算机系统接口）协议，主要提供块设备服务。
2）NAS协议：可以运行在以太网上提供文件服务。
3）Restful协议：运行在HTTP之上的（http协议亦可以工作在以太网上），提供相关对象存储服务。
4）运行在以太网协议的FC（Fibre Chanel）协议FCOE：用以

利用DPDK来优化网络驱动，通过无锁硬件访问技术加速报文处理，例如Intel网卡的Flow Director技术将指定的TCP流导入到特定的硬件队列，针对每个Connection（连接）分配不同的TX/RX队列。

DPDK只提供了对OSI 7层协议的第2层和第3层的支持，为此需要第4层以上协议（主要是需要TCP或者UDP）支持的应用程序无法直接使用DPDK。为了弥补这一个问题，SPDK中提供了一个用户态的TCP/IP协议栈（libuns）。为了高效地利用DPDK对于网络报文快速处理这一特性，一个有效的网络协议栈必须解决兼容性和性能问题。

为了更好地发挥DPDK的优势，SPDK提供了一些用户态的存储驱动，诸如基于NVME协议的SSD的用户态驱动。NVME（Non-Volatile Memory Express）是一个基于PCI-e接口的协议。通过优化存储驱动，再配合经过DPDK优化的用户态协议栈，存储网络服务器端的CPU资源占用将被进一步降低。


